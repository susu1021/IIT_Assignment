# -*- coding: utf-8 -*-
"""PR_assignment4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TMWSUQOdhUymsImQEbyMDu6iCCcw7Hfr
"""

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split

# Define dataset directory
dataset_dir = '/content/Dataset'

# Define the number of classes and images per class
num_classes = 2
images_per_class = 5

# Define image dimensions for resizing
img_width = 100
img_height = 100

# Define empty arrays for images and labels
images = []
labels = []

# Iterate over each class in the dataset directory
for i in range(num_classes):
    class_dir = os.path.join(dataset_dir, f'class_{i+1}')

    # Check if the class directory exists
    if not os.path.isdir(class_dir):
        print(f"Class directory {class_dir} does not exist.")
        continue

    # Iterate over each image in the class directory
    for j in range(images_per_class):
        img_path = os.path.join(class_dir, f'{j+1}.jpeg')

        # Check if the image file exists
        if not os.path.isfile(img_path):
            print(f"Image file {img_path} does not exist.")
            continue

        # Read image
        img = cv2.imread(img_path)

        # Check if the image was read correctly
        if img is None:
            print(f"Failed to read the image at {img_path}.")
            continue

        # Resize the image
        try:
            img = cv2.resize(img, (img_width, img_height))
        except cv2.error as e:
            print(f"Error resizing image {img_path}: {e}")
            continue

        # Convert to grayscale
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # Apply normalization if needed
        img = img / 255.0

        # Append image and label to respective arrays
        images.append(img)
        labels.append(i)

# Convert images and labels to numpy arrays
images = np.array(images)
labels = np.array(labels)

# Check if the arrays are empty
if images.size == 0 or labels.size == 0:
    print("No images or labels were loaded.")
else:
    # Split the dataset into training and testing sets
    train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.4, random_state=42)

    # Print the shape of the training and testing sets
    print('Training Images:', train_images.shape)
    print('Training Labels:', train_labels.shape)
    print('Testing Images:', test_images.shape)
    print('Testing Labels:', test_labels.shape)





import numpy as np
import cv2
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Define image dimensions for resizing
img_width = 50
img_height = 50

# Load the training data
train_images = []
for i in range(1):
    for j in range(3):
        img_path = f'/content/Dataset/class_1/Training/{i}_{j}.jpeg'

        # Read the image in grayscale mode
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)

        # Check if the image was read correctly
        if img is None:
            print(f"Failed to read the image at {img_path}.")
            continue

        # Resize the image
        try:
            img = cv2.resize(img, (img_width, img_height))
        except cv2.error as e:
            print(f"Error resizing image {img_path}: {e}")
            continue

        # Append the image to the list
        train_images.append(img)

# Check if any images were loaded
if len(train_images) == 0:
    print("No images were loaded. Exiting.")
    exit()

# Convert the list to a numpy array
train_images = np.array(train_images)

# Flatten the images into 1D arrays
train_images_flat = train_images.reshape(train_images.shape[0], -1)

# Perform PCA on the flattened training images
pca = PCA()
pca.fit(train_images_flat)

# Plot the explained variance ratio vs the number of principal components
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Principal Components')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance Ratio vs Number of Principal Components')
plt.show()

# Print the number of components
num_available_components = pca.components_.shape[0]
print(f"Number of available principal components: {num_available_components}")

# Visualize the first 10, 50, and 100 principal components
num_components = [10, 50, 100]
for n in num_components:
    n = min(n, num_available_components)  # Ensure n does not exceed the available number of components
    fig, axs = plt.subplots(1, n, figsize=(n*2, 2))
    for i in range(n):
        pc = pca.components_[i].reshape(img_height, img_width)
        axs[i].imshow(pc, cmap='gray')
        axs[i].axis('off')
    plt.suptitle(f'First {n} Principal Components')
    plt.show()

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Path to the dataset
dataset_path = '/content/dataset'

# Function to load images
def load_images_from_folder(folder):
    images = []
    for filename in sorted(os.listdir(folder))[:5]:  # Ensure the first 5 images per class
        img = cv2.imread(os.path.join(folder, filename), cv2.IMREAD_GRAYSCALE)
        if img is not None:
            img = cv2.resize(img, (28, 28))  # Resize all images to 28x28
            images.append(img)
    return images

# Load dataset
X = []
y = []
class_labels = sorted(os.listdir(dataset_path))
for label in class_labels:
    folder_path = os.path.join(dataset_path, label)
    images = load_images_from_folder(folder_path)
    X.extend(images)
    y.extend([label] * len(images))

X = np.array(X)
y = np.array(y)

# Normalize images
X = X / 255.0

# Reshape images to (num_samples, num_features)
X = X.reshape(X.shape[0], -1)

# Split the dataset into training and testing sets
X_train, y_train = X[:3*10], y[:3*10]
X_test, y_test = X[3*10:], y[3*10:]

# Print shapes
print(f"Training set shape: {X_train.shape}, {y_train.shape}")
print(f"Testing set shape: {X_test.shape}, {y_test.shape}")

# Implement PCA
def apply_pca(n_components, X_train, X_test):
    pca = PCA(n_components=n_components)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)
    return X_train_pca, X_test_pca, pca

# Implement LDA
def apply_lda(n_components, X_train, y_train, X_test):
    lda = LDA(n_components=n_components)
    X_train_lda = lda.fit_transform(X_train, y_train)
    X_test_lda = lda.transform(X_test)
    return X_train_lda, X_test_lda, lda

# Analyze the effect of varying the number of principal components
n_components_list = [10, 50, 100]
for n_components in n_components_list:
    X_train_pca, X_test_pca, pca = apply_pca(n_components, X_train, X_test)
    print(f"PCA with {n_components} components")

    # Visualize the first n components
    plt.figure(figsize=(8, 4))
    for i in range(1, 11):
        plt.subplot(2, 5, i)
        plt.imshow(pca.components_[i-1].reshape(28, 28), cmap='gray')
        plt.title(f'Component {i}')
    plt.show()

# Analyze the effect of varying the number of discriminant features
n_components_list_lda = [1, 2, 9]
for n_components in n_components_list_lda:
    X_train_lda, X_test_lda, lda = apply_lda(n_components, X_train, y_train, X_test)
    print(f"LDA with {n_components} components")

    # Visualize the first n discriminant features
    plt.figure(figsize=(8, 4))
    for i in range(1, n_components + 1):
        plt.subplot(1, n_components, i)
        plt.imshow(lda.scalings_[:, i-1].reshape(28, 28), cmap='gray')
        plt.title(f'Component {i}')
    plt.show()

# Build and evaluate the Template Matching system using KNN
def evaluate_model(X_train, y_train, X_test, y_test):
    model = KNeighborsClassifier(n_neighbors=3)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')

    return accuracy, precision, recall, f1

# Evaluation for PCA
results_pca = {}
for n_components in n_components_list:
    X_train_pca, X_test_pca, _ = apply_pca(n_components, X_train, X_test)
    accuracy, precision, recall, f1 = evaluate_model(X_train_pca, y_train, X_test_pca, y_test)
    results_pca[n_components] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}

# Evaluation for LDA
results_lda = {}
for n_components in n_components_list_lda:
    X_train_lda, X_test_lda, _ = apply_lda(n_components, X_train, y_train, X_test)
    accuracy, precision, recall, f1 = evaluate_model(X_train_lda, y_train, X_test_lda, y_test)
    results_lda[n_components] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}

# Display results
print("PCA Results:")
for n_components, metrics in results_pca.items():
    print(f"Components: {n_components}, Accuracy: {metrics['accuracy']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1 Score: {metrics['f1']:.4f}")

print("\nLDA Results:")
for n_components, metrics in results_lda.items():
    print(f"Components: {n_components}, Accuracy: {metrics['accuracy']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1 Score: {metrics['f1']:.4f}")

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Path to the dataset
dataset_path = '/content/dataset'

# Function to load images
def load_images_from_folder(folder):
    images = []
    for filename in sorted(os.listdir(folder))[:5]:  # Ensure the first 5 images per class
        img = cv2.imread(os.path.join(folder, filename), cv2.IMREAD_GRAYSCALE)
        if img is not None:
            img = cv2.resize(img, (28, 28))  # Resize all images to 28x28
            images.append(img)
    return images

# Load dataset
X = []
y = []
class_labels = sorted(os.listdir(dataset_path))
for label in class_labels:
    folder_path = os.path.join(dataset_path, label)
    images = load_images_from_folder(folder_path)
    X.extend(images)
    y.extend([label] * len(images))

X = np.array(X)
y = np.array(y)

# Normalize images
X = X / 255.0

# Reshape images to (num_samples, num_features)
X = X.reshape(X.shape[0], -1)

# Split the dataset into training and testing sets
X_train, y_train = X[:3*2], y[:3*2]
X_test, y_test = X[3*2:], y[3*2:]

# Print shapes
print(f"Training set shape: {X_train.shape}, {y_train.shape}")
print(f"Testing set shape: {X_test.shape}, {y_test.shape}")

# Implement PCA
def apply_pca(n_components, X_train, X_test):
    pca = PCA(n_components=n_components)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)
    return X_train_pca, X_test_pca, pca

# Implement LDA
def apply_lda(n_components, X_train, y_train, X_test):
    lda = LDA(n_components=n_components)
    X_train_lda = lda.fit_transform(X_train, y_train)
    X_test_lda = lda.transform(X_test)
    return X_train_lda, X_test_lda, lda

# Analyze the effect of varying the number of principal components
n_components_list = [2]  # Reduced to 2 due to having only 2 classes
for n_components in n_components_list:
    X_train_pca, X_test_pca, pca = apply_pca(n_components, X_train, X_test)
    print(f"PCA with {n_components} components")

    # Visualize the first n components
    plt.figure(figsize=(8, 4))
    for i in range(1, n_components + 1):
        plt.subplot(1, n_components, i)
        plt.imshow(pca.components_[i-1].reshape(28, 28), cmap='gray')
        plt.title(f'Component {i}')
    plt.show()

# Analyze the effect of varying the number of discriminant features
n_components_list_lda = [1]  # Maximum is 1 because of 2 classes (n_classes - 1)
for n_components in n_components_list_lda:
    X_train_lda, X_test_lda, lda = apply_lda(n_components, X_train, y_train, X_test)
    print(f"LDA with {n_components} components")

    # Visualize the first n discriminant features
    plt.figure(figsize=(4, 4))
    plt.imshow(lda.scalings_[:, 0].reshape(28, 28), cmap='gray')
    plt.title('Component 1')
    plt.show()

# Build and evaluate the Template Matching system using KNN
def evaluate_model(X_train, y_train, X_test, y_test):
    model = KNeighborsClassifier(n_neighbors=3)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')

    return accuracy, precision, recall, f1

# Evaluation for PCA
results_pca = {}
for n_components in n_components_list:
    X_train_pca, X_test_pca, _ = apply_pca(n_components, X_train, X_test)
    accuracy, precision, recall, f1 = evaluate_model(X_train_pca, y_train, X_test_pca, y_test)
    results_pca[n_components] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}

# Evaluation for LDA
results_lda = {}
for n_components in n_components_list_lda:
    X_train_lda, X_test_lda, _ = apply_lda(n_components, X_train, y_train, X_test)
    accuracy, precision, recall, f1 = evaluate_model(X_train_lda, y_train, X_test_lda, y_test)
    results_lda[n_components] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}

# Display results
print("PCA Results:")
for n_components, metrics in results_pca.items():
    print(f"Components: {n_components}, Accuracy: {metrics['accuracy']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1 Score: {metrics['f1']:.4f}")

print("\nLDA Results:")
for n_components, metrics in results_lda.items():
    print(f"Components: {n_components}, Accuracy: {metrics['accuracy']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1 Score: {metrics['f1']:.4f}")

# Compare the performance of PCA and LDA
print("\nComparison of PCA and LDA:")
for n_components in n_components_list:
    pca_metrics = results_pca[n_components]
    print(f"PCA with {n_components} components -> Accuracy: {pca_metrics['accuracy']:.4f}, Precision: {pca_metrics['precision']:.4f}, Recall: {pca_metrics['recall']:.4f}, F1 Score: {pca_metrics['f1']:.4f}")

for n_components in n_components_list_lda:
    lda_metrics = results_lda[n_components]
    print(f"LDA with {n_components} components -> Accuracy: {lda_metrics['accuracy']:.4f}, Precision: {lda_metrics['precision']:.4f}, Recall: {lda_metrics['recall']:.4f}, F1 Score: {lda_metrics['f1']:.4f}")

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Path to the dataset
dataset_path = '/content/dataset'

# Function to load images
def load_images_from_folder(folder):
    images = []
    for filename in sorted(os.listdir(folder)):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is not None:
            img = cv2.resize(img, (28, 28))  # Resize all images to 28x28
            images.append(img)
    return images

# Load dataset
X = []
y = []
class_labels = sorted(os.listdir(dataset_path))
for idx, label in enumerate(class_labels):
    folder_path = os.path.join(dataset_path, label)
    images = load_images_from_folder(folder_path)
    X.extend(images)
    y.extend([idx] * len(images))  # Assign numeric labels

X = np.array(X)
y = np.array(y)

# Normalize images
X = X / 255.0

# Reshape images to (num_samples, num_features)
X = X.reshape(X.shape[0], -1)

# Split the dataset into training and testing sets
X_train, y_train = [], []
X_test, y_test = [], []

num_train = 3  # First 3 images for training
num_test = 2  # Last 2 images for testing

for idx in range(len(class_labels)):
    start_idx = idx * (num_train + num_test)
    X_train.extend(X[start_idx:start_idx + num_train])
    y_train.extend(y[start_idx:start_idx + num_train])
    X_test.extend(X[start_idx + num_train:start_idx + num_train + num_test])
    y_test.extend(y[start_idx + num_train:start_idx + num_train + num_test])

X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)

# Print shapes
print(f"Training set shape: {X_train.shape}, {y_train.shape}")
print(f"Testing set shape: {X_test.shape}, {y_test.shape}")

# Implement PCA
def apply_pca(n_components, X_train, X_test):
    pca = PCA(n_components=n_components)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)
    return X_train_pca, X_test_pca, pca

# Implement LDA
def apply_lda(n_components, X_train, y_train, X_test):
    lda = LDA(n_components=n_components)
    X_train_lda = lda.fit_transform(X_train, y_train)
    X_test_lda = lda.transform(X_test)
    return X_train_lda, X_test_lda, lda

# Analyze the effect of varying the number of principal components
n_components_list = [2]  # Reduced to 2 due to having only 2 classes
for n_components in n_components_list:
    X_train_pca, X_test_pca, pca = apply_pca(n_components, X_train, X_test)
    print(f"PCA with {n_components} components")

    # Visualize the first n components
    plt.figure(figsize=(8, 4))
    for i in range(1, n_components + 1):
        plt.subplot(1, n_components, i)
        plt.imshow(pca.components_[i-1].reshape(28, 28), cmap='gray')
        plt.title(f'Component {i}')
    plt.show()

# Analyze the effect of varying the number of discriminant features
n_components_list_lda = [1]  # Maximum is 1 because of 2 classes (n_classes - 1)
for n_components in n_components_list_lda:
    X_train_lda, X_test_lda, lda = apply_lda(n_components, X_train, y_train, X_test)
    print(f"LDA with {n_components} components")

    # Visualize the first n discriminant features
    plt.figure(figsize=(4, 4))
    plt.imshow(lda.scalings_[:, 0].reshape(28, 28), cmap='gray')
    plt.title('Component 1')
    plt.show()

# Build and evaluate the Template Matching system using KNN
def evaluate_model(X_train, y_train, X_test, y_test):
    model = KNeighborsClassifier(n_neighbors=3)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')

    return accuracy, precision, recall, f1

# Evaluation for PCA
results_pca = {}
for n_components in n_components_list:
    X_train_pca, X_test_pca, _ = apply_pca(n_components, X_train, X_test)
    accuracy, precision, recall, f1 = evaluate_model(X_train_pca, y_train, X_test_pca, y_test)
    results_pca[n_components] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}

# Evaluation for LDA
results_lda = {}
for n_components in n_components_list_lda:
    X_train_lda, X_test_lda, _ = apply_lda(n_components, X_train, y_train, X_test)
    accuracy, precision, recall, f1 = evaluate_model(X_train_lda, y_train, X_test_lda, y_test)
    results_lda[n_components] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}

# Display results
print("PCA Results:")
for n_components, metrics in results_pca.items():
    print(f"Components: {n_components}, Accuracy: {metrics['accuracy']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1 Score: {metrics['f1']:.4f}")

print("\nLDA Results:")
for n_components, metrics in results_lda.items():
    print(f"Components: {n_components}, Accuracy: {metrics['accuracy']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1 Score: {metrics['f1']:.4f}")

# Compare the performance of PCA and LDA
print("\nComparison of PCA and LDA:")
for n_components in n_components_list:
    pca_metrics = results_pca[n_components]
    print(f"PCA with {n_components} components -> Accuracy: {pca_metrics['accuracy']:.4f}, Precision: {pca_metrics['precision']:.4f}, Recall: {pca_metrics['recall']:.4f}, F1 Score: {pca_metrics['f1']:.4f}")

for n_components in n_components_list_lda:
    lda_metrics = results_lda[n_components]
    print(f"LDA with {n_components} components -> Accuracy: {lda_metrics['accuracy']:.4f}, Precision: {lda_metrics['precision']:.4f}, Recall: {lda_metrics['recall']:.4f}, F1 Score: {lda_metrics['f1']:.4f}")

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Path to the dataset
dataset_path = '/content/dataset'

# Function to load images
def load_images_from_folder(folder):
    images = []
    for filename in sorted(os.listdir(folder)):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is not None:
            img = cv2.resize(img, (28, 28))  # Resize all images to 28x28
            images.append(img)
    return images

# Load dataset
X = []
y = []
class_labels = sorted(os.listdir(dataset_path))
for idx, label in enumerate(class_labels):
    folder_path = os.path.join(dataset_path, label)
    images = load_images_from_folder(folder_path)
    X.extend(images)
    y.extend([idx] * len(images))  # Assign numeric labels

X = np.array(X)
y = np.array(y)

# Normalize images
X = X / 255.0

# Reshape images to (num_samples, num_features)
X = X.reshape(X.shape[0], -1)

# Split the dataset into training and testing sets
X_train, y_train = [], []
X_test, y_test = [], []

num_train = 3  # First 3 images for training
num_test = 2  # Last 2 images for testing

for idx in range(len(class_labels)):
    class_indices = np.where(y == idx)[0]
    X_train.extend(X[class_indices[:num_train]])
    y_train.extend(y[class_indices[:num_train]])
    X_test.extend(X[class_indices[num_train:num_train + num_test]])
    y_test.extend(y[class_indices[num_train:num_train + num_test]])

X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)

# Print shapes
print(f"Training set shape: {X_train.shape}, {y_train.shape}")
print(f"Testing set shape: {X_test.shape}, {y_test.shape}")

# Implement PCA
def apply_pca(n_components, X_train, X_test):
    pca = PCA(n_components=n_components)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)
    return X_train_pca, X_test_pca, pca

# Implement LDA
def apply_lda(n_components, X_train, y_train, X_test):
    lda = LDA(n_components=n_components)
    X_train_lda = lda.fit_transform(X_train, y_train)
    X_test_lda = lda.transform(X_test)
    return X_train_lda, X_test_lda, lda

# Analyze the effect of varying the number of principal components
n_components_list = [2]  # Reduced to 2 due to having only 2 classes
for n_components in n_components_list:
    X_train_pca, X_test_pca, pca = apply_pca(n_components, X_train, X_test)
    print(f"PCA with {n_components} components")

    # Visualize the first n components
    plt.figure(figsize=(8, 4))
    for i in range(1, n_components + 1):
        plt.subplot(1, n_components, i)
        plt.imshow(pca.components_[i-1].reshape(28, 28), cmap='gray')
        plt.title(f'Component {i}')
    plt.show()

# Analyze the effect of varying the number of discriminant features
n_components_list_lda = [1]  # Maximum is 1 because of 2 classes (n_classes - 1)
for n_components in n_components_list_lda:
    X_train_lda, X_test_lda, lda = apply_lda(n_components, X_train, y_train, X_test)
    print(f"LDA with {n_components} components")

    # Visualize the first n discriminant features
    plt.figure(figsize=(4, 4))
    plt.imshow(lda.scalings_[:, 0].reshape(28, 28), cmap='gray')
    plt.title('Component 1')
    plt.show()

# Build and evaluate the Template Matching system using KNN
def evaluate_model(X_train, y_train, X_test, y_test):
    model = KNeighborsClassifier(n_neighbors=3)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')

    return accuracy, precision, recall, f1

# Evaluation for PCA
results_pca = {}
for n_components in n_components_list:
    X_train_pca, X_test_pca, _ = apply_pca(n_components, X_train, X_test)
    accuracy, precision, recall, f1 = evaluate_model(X_train_pca, y_train, X_test_pca, y_test)
    results_pca[n_components] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}

# Evaluation for LDA
results_lda = {}
for n_components in n_components_list_lda:
    X_train_lda, X_test_lda, _ = apply_lda(n_components, X_train, y_train, X_test)
    accuracy, precision, recall, f1 = evaluate_model(X_train_lda, y_train, X_test_lda, y_test)
    results_lda[n_components] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}

# Display results
print("PCA Results:")
for n_components, metrics in results_pca.items():
    print(f"Components: {n_components}, Accuracy: {metrics['accuracy']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1 Score: {metrics['f1']:.4f}")

print("\nLDA Results:")
for n_components, metrics in results_lda.items():
    print(f"Components: {n_components}, Accuracy: {metrics['accuracy']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1 Score: {metrics['f1']:.4f}")

# Compare the performance of PCA and LDA
print("\nComparison of PCA and LDA:")
for n_components in n_components_list:
    pca_metrics = results_pca[n_components]
    print(f"PCA with {n_components} components -> Accuracy: {pca_metrics['accuracy']:.4f}, Precision: {pca_metrics['precision']:.4f}, Recall: {pca_metrics['recall']:.4f}, F1 Score: {pca_metrics['f1']:.4f}")

for n_components in n_components_list_lda:
    lda_metrics = results_lda[n_components]
    print(f"LDA with {n_components} components -> Accuracy: {lda_metrics['accuracy']:.4f}, Precision: {lda_metrics['precision']:.4f}, Recall: {lda_metrics['recall']:.4f}, F1 Score: {lda_metrics['f1']:.4f}")